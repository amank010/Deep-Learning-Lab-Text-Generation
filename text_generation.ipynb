{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uGZ33t48yCx2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Load the Dataset\n",
        "text = \"\"\n",
        "file_path = \"/content/poems-100.csv\"  # Adjust this if needed\n",
        "\n",
        "with open(file_path, \"r\") as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        text += \" \".join(row) + \" \""
      ],
      "metadata": {
        "id": "vLXF2sy1yEMu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"imbikramsaha/poems\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55HNCUkJyI-R",
        "outputId": "302760d3-e055-407a-da02-581cf54ff9d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/imbikramsaha/poems?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 57.6k/57.6k [00:00<00:00, 42.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/imbikramsaha/poems/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the Text into Words\n",
        "tokens = text.split()"
      ],
      "metadata": {
        "id": "C7wHRIXwynh8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dictionary to Map Words to Indices\n",
        "word_to_idx = {}\n",
        "idx_to_word = {}\n",
        "vocab_size = 0\n",
        "\n",
        "for word in tokens:\n",
        "    if word not in word_to_idx:\n",
        "        word_to_idx[word] = vocab_size\n",
        "        idx_to_word[vocab_size] = word\n",
        "        vocab_size += 1"
      ],
      "metadata": {
        "id": "rQXjxBf5zho2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Tokens to Indices\n",
        "token_indices = [word_to_idx[word] for word in tokens]"
      ],
      "metadata": {
        "id": "tl_Zlnm-ziyL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Sequences and Targets\n",
        "seq_length = 10\n",
        "sequences = []\n",
        "targets = []\n",
        "\n",
        "for i in range(len(token_indices) - seq_length):\n",
        "    seq = token_indices[i:i + seq_length]\n",
        "    target = token_indices[i + seq_length]\n",
        "    sequences.append(seq)\n",
        "    targets.append(target)"
      ],
      "metadata": {
        "id": "DbsNFckLzkC2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch Tensors\n",
        "sequences = torch.tensor(sequences, dtype = torch.long)\n",
        "targets = torch.tensor(targets, dtype = torch.long)"
      ],
      "metadata": {
        "id": "RO6vrHJ9zk1L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define One-Hot Encoding for RNN Model\n",
        "class OneHotRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, output_dim):\n",
        "        super(OneHotRNN, self).__init__()\n",
        "        self.rnn = nn.RNN(vocab_size, hidden_dim, batch_first = True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, _ = self.rnn(x)\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "C4zqc0r8zlyW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LSTM Model with Embedding Layer\n",
        "class PoemLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(PoemLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first = True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "kZx4mxGdzmp2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embed_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = vocab_size"
      ],
      "metadata": {
        "id": "ESveVdYFznl7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Models\n",
        "onehot_model = OneHotRNN(vocab_size, hidden_dim, output_dim)\n",
        "embedding_model = PoemLSTM(vocab_size, embed_dim, hidden_dim, output_dim)"
      ],
      "metadata": {
        "id": "yR_PWZm6zuTE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "onehot_optimizer = optim.Adam(onehot_model.parameters(), lr = 0.001)\n",
        "embedding_optimizer = optim.Adam(embedding_model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "WvhvCWFHzvUf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Tracking\n",
        "onehot_losses, embedding_losses = [], []"
      ],
      "metadata": {
        "id": "Cd-XmTLFzwJO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function with Tracking\n",
        "def train_model(model, optimizer, name):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(100):\n",
        "        total_loss = 0\n",
        "        for i in range(0, len(sequences), 32):\n",
        "            batch_seq = sequences[i:i + 32]\n",
        "            batch_target = targets[i:i + 32]\n",
        "\n",
        "            # One-Hot Encoding for OneHotRNN\n",
        "            if name == \"OneHotRNN\":\n",
        "                batch_seq = F.one_hot(batch_seq, num_classes = vocab_size).float()\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = model(batch_seq)\n",
        "            loss = criterion(outputs, batch_target)\n",
        "\n",
        "            # Backward Pass and Optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / (len(sequences) // 32)\n",
        "        if name == \"OneHotRNN\":\n",
        "            onehot_losses.append(avg_loss)\n",
        "        else:\n",
        "            embedding_losses.append(avg_loss)\n",
        "\n",
        "        print(f\"{name} Epoch [{epoch+1}/100], Avg Loss: {avg_loss:.4f}\")\n",
        "    print(f\"{name} Training Time: {time.time() - start_time:.2f}s\\n\")"
      ],
      "metadata": {
        "id": "XnkjE_SUzyMz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Poem Generation Function\n",
        "def generate_poem(model, seed_text, num_words = 50, model_type = \"EmbeddingLSTM\"):\n",
        "    model.eval()\n",
        "    words = seed_text.split()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_words):\n",
        "            seq = [word_to_idx.get(word, 0) for word in words[-seq_length:]]\n",
        "            seq = torch.tensor(seq, dtype = torch.long).unsqueeze(0)\n",
        "\n",
        "            if model_type == \"OneHotRNN\":\n",
        "                seq = F.one_hot(seq, num_classes = vocab_size).float()\n",
        "\n",
        "            output = model(seq)\n",
        "            probabilities = F.softmax(output, dim = 1)\n",
        "            predicted_idx = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            words.append(idx_to_word[predicted_idx])\n",
        "\n",
        "    return \" \".join(words)"
      ],
      "metadata": {
        "id": "1Ou_-M-x2pvf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Models\n",
        "train_model(onehot_model, onehot_optimizer, \"OneHotRNN\")\n",
        "train_model(embedding_model, embedding_optimizer, \"EmbeddingLSTM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4pja4u62tGe",
        "outputId": "44834996-edd9-407f-852e-ac63b9933242"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OneHotRNN Epoch [1/100], Avg Loss: 7.5809\n",
            "OneHotRNN Epoch [2/100], Avg Loss: 6.6965\n",
            "OneHotRNN Epoch [3/100], Avg Loss: 6.3395\n",
            "OneHotRNN Epoch [4/100], Avg Loss: 6.1563\n",
            "OneHotRNN Epoch [5/100], Avg Loss: 5.9387\n",
            "OneHotRNN Epoch [6/100], Avg Loss: 5.7972\n",
            "OneHotRNN Epoch [7/100], Avg Loss: 5.6211\n",
            "OneHotRNN Epoch [8/100], Avg Loss: 5.3685\n",
            "OneHotRNN Epoch [9/100], Avg Loss: 5.1312\n",
            "OneHotRNN Epoch [10/100], Avg Loss: 4.9061\n",
            "OneHotRNN Epoch [11/100], Avg Loss: 4.6794\n",
            "OneHotRNN Epoch [12/100], Avg Loss: 4.4024\n",
            "OneHotRNN Epoch [13/100], Avg Loss: 4.1496\n",
            "OneHotRNN Epoch [14/100], Avg Loss: 3.8914\n",
            "OneHotRNN Epoch [15/100], Avg Loss: 3.5549\n",
            "OneHotRNN Epoch [16/100], Avg Loss: 3.2254\n",
            "OneHotRNN Epoch [17/100], Avg Loss: 2.9756\n",
            "OneHotRNN Epoch [18/100], Avg Loss: 2.7255\n",
            "OneHotRNN Epoch [19/100], Avg Loss: 2.4504\n",
            "OneHotRNN Epoch [20/100], Avg Loss: 2.1931\n",
            "OneHotRNN Epoch [21/100], Avg Loss: 1.9304\n",
            "OneHotRNN Epoch [22/100], Avg Loss: 1.6641\n",
            "OneHotRNN Epoch [23/100], Avg Loss: 1.4530\n",
            "OneHotRNN Epoch [24/100], Avg Loss: 1.2603\n",
            "OneHotRNN Epoch [25/100], Avg Loss: 1.0864\n",
            "OneHotRNN Epoch [26/100], Avg Loss: 0.9164\n",
            "OneHotRNN Epoch [27/100], Avg Loss: 0.7612\n",
            "OneHotRNN Epoch [28/100], Avg Loss: 0.6237\n",
            "OneHotRNN Epoch [29/100], Avg Loss: 0.5050\n",
            "OneHotRNN Epoch [30/100], Avg Loss: 0.4101\n",
            "OneHotRNN Epoch [31/100], Avg Loss: 0.3394\n",
            "OneHotRNN Epoch [32/100], Avg Loss: 0.2773\n",
            "OneHotRNN Epoch [33/100], Avg Loss: 0.2295\n",
            "OneHotRNN Epoch [34/100], Avg Loss: 0.1946\n",
            "OneHotRNN Epoch [35/100], Avg Loss: 0.1657\n",
            "OneHotRNN Epoch [36/100], Avg Loss: 0.1411\n",
            "OneHotRNN Epoch [37/100], Avg Loss: 0.1241\n",
            "OneHotRNN Epoch [38/100], Avg Loss: 0.1115\n",
            "OneHotRNN Epoch [39/100], Avg Loss: 0.1036\n",
            "OneHotRNN Epoch [40/100], Avg Loss: 0.0973\n",
            "OneHotRNN Epoch [41/100], Avg Loss: 0.0930\n",
            "OneHotRNN Epoch [42/100], Avg Loss: 0.0803\n",
            "OneHotRNN Epoch [43/100], Avg Loss: 0.0723\n",
            "OneHotRNN Epoch [44/100], Avg Loss: 0.0673\n",
            "OneHotRNN Epoch [45/100], Avg Loss: 0.0525\n",
            "OneHotRNN Epoch [46/100], Avg Loss: 0.0475\n",
            "OneHotRNN Epoch [47/100], Avg Loss: 0.0422\n",
            "OneHotRNN Epoch [48/100], Avg Loss: 0.0390\n",
            "OneHotRNN Epoch [49/100], Avg Loss: 0.0413\n",
            "OneHotRNN Epoch [50/100], Avg Loss: 0.0445\n",
            "OneHotRNN Epoch [51/100], Avg Loss: 0.0400\n",
            "OneHotRNN Epoch [52/100], Avg Loss: 0.0343\n",
            "OneHotRNN Epoch [53/100], Avg Loss: 0.0278\n",
            "OneHotRNN Epoch [54/100], Avg Loss: 0.0255\n",
            "OneHotRNN Epoch [55/100], Avg Loss: 0.0236\n",
            "OneHotRNN Epoch [56/100], Avg Loss: 0.0348\n",
            "OneHotRNN Epoch [57/100], Avg Loss: 0.0289\n",
            "OneHotRNN Epoch [58/100], Avg Loss: 0.0246\n",
            "OneHotRNN Epoch [59/100], Avg Loss: 0.0211\n",
            "OneHotRNN Epoch [60/100], Avg Loss: 0.0201\n",
            "OneHotRNN Epoch [61/100], Avg Loss: 0.0215\n",
            "OneHotRNN Epoch [62/100], Avg Loss: 0.0235\n",
            "OneHotRNN Epoch [63/100], Avg Loss: 0.0213\n",
            "OneHotRNN Epoch [64/100], Avg Loss: 0.0238\n",
            "OneHotRNN Epoch [65/100], Avg Loss: 0.0181\n",
            "OneHotRNN Epoch [66/100], Avg Loss: 0.0188\n",
            "OneHotRNN Epoch [67/100], Avg Loss: 0.0182\n",
            "OneHotRNN Epoch [68/100], Avg Loss: 0.0200\n",
            "OneHotRNN Epoch [69/100], Avg Loss: 0.0217\n",
            "OneHotRNN Epoch [70/100], Avg Loss: 0.0188\n",
            "OneHotRNN Epoch [71/100], Avg Loss: 0.0194\n",
            "OneHotRNN Epoch [72/100], Avg Loss: 0.0214\n",
            "OneHotRNN Epoch [73/100], Avg Loss: 0.0150\n",
            "OneHotRNN Epoch [74/100], Avg Loss: 0.0148\n",
            "OneHotRNN Epoch [75/100], Avg Loss: 0.0129\n",
            "OneHotRNN Epoch [76/100], Avg Loss: 0.0223\n",
            "OneHotRNN Epoch [77/100], Avg Loss: 0.0167\n",
            "OneHotRNN Epoch [78/100], Avg Loss: 0.0134\n",
            "OneHotRNN Epoch [79/100], Avg Loss: 0.0165\n",
            "OneHotRNN Epoch [80/100], Avg Loss: 0.0118\n",
            "OneHotRNN Epoch [81/100], Avg Loss: 0.0193\n",
            "OneHotRNN Epoch [82/100], Avg Loss: 0.0178\n",
            "OneHotRNN Epoch [83/100], Avg Loss: 0.0101\n",
            "OneHotRNN Epoch [84/100], Avg Loss: 0.0115\n",
            "OneHotRNN Epoch [85/100], Avg Loss: 0.0184\n",
            "OneHotRNN Epoch [86/100], Avg Loss: 0.0192\n",
            "OneHotRNN Epoch [87/100], Avg Loss: 0.0102\n",
            "OneHotRNN Epoch [88/100], Avg Loss: 0.0091\n",
            "OneHotRNN Epoch [89/100], Avg Loss: 0.0136\n",
            "OneHotRNN Epoch [90/100], Avg Loss: 0.0182\n",
            "OneHotRNN Epoch [91/100], Avg Loss: 0.0103\n",
            "OneHotRNN Epoch [92/100], Avg Loss: 0.0051\n",
            "OneHotRNN Epoch [93/100], Avg Loss: 0.0216\n",
            "OneHotRNN Epoch [94/100], Avg Loss: 0.0147\n",
            "OneHotRNN Epoch [95/100], Avg Loss: 0.0080\n",
            "OneHotRNN Epoch [96/100], Avg Loss: 0.0176\n",
            "OneHotRNN Epoch [97/100], Avg Loss: 0.0147\n",
            "OneHotRNN Epoch [98/100], Avg Loss: 0.0060\n",
            "OneHotRNN Epoch [99/100], Avg Loss: 0.0026\n",
            "OneHotRNN Epoch [100/100], Avg Loss: 0.0015\n",
            "OneHotRNN Training Time: 4530.99s\n",
            "\n",
            "EmbeddingLSTM Epoch [1/100], Avg Loss: 7.5753\n",
            "EmbeddingLSTM Epoch [2/100], Avg Loss: 6.5845\n",
            "EmbeddingLSTM Epoch [3/100], Avg Loss: 5.9915\n",
            "EmbeddingLSTM Epoch [4/100], Avg Loss: 5.4871\n",
            "EmbeddingLSTM Epoch [5/100], Avg Loss: 4.9840\n",
            "EmbeddingLSTM Epoch [6/100], Avg Loss: 4.5461\n",
            "EmbeddingLSTM Epoch [7/100], Avg Loss: 3.9466\n",
            "EmbeddingLSTM Epoch [8/100], Avg Loss: 3.4007\n",
            "EmbeddingLSTM Epoch [9/100], Avg Loss: 2.9207\n",
            "EmbeddingLSTM Epoch [10/100], Avg Loss: 2.5074\n",
            "EmbeddingLSTM Epoch [11/100], Avg Loss: 2.1605\n",
            "EmbeddingLSTM Epoch [12/100], Avg Loss: 1.8765\n",
            "EmbeddingLSTM Epoch [13/100], Avg Loss: 1.6308\n",
            "EmbeddingLSTM Epoch [14/100], Avg Loss: 1.4194\n",
            "EmbeddingLSTM Epoch [15/100], Avg Loss: 1.2328\n",
            "EmbeddingLSTM Epoch [16/100], Avg Loss: 1.0841\n",
            "EmbeddingLSTM Epoch [17/100], Avg Loss: 0.9540\n",
            "EmbeddingLSTM Epoch [18/100], Avg Loss: 0.8453\n",
            "EmbeddingLSTM Epoch [19/100], Avg Loss: 0.7363\n",
            "EmbeddingLSTM Epoch [20/100], Avg Loss: 0.6527\n",
            "EmbeddingLSTM Epoch [21/100], Avg Loss: 0.5773\n",
            "EmbeddingLSTM Epoch [22/100], Avg Loss: 0.5074\n",
            "EmbeddingLSTM Epoch [23/100], Avg Loss: 0.4615\n",
            "EmbeddingLSTM Epoch [24/100], Avg Loss: 0.4375\n",
            "EmbeddingLSTM Epoch [25/100], Avg Loss: 0.4253\n",
            "EmbeddingLSTM Epoch [26/100], Avg Loss: 0.4177\n",
            "EmbeddingLSTM Epoch [27/100], Avg Loss: 0.4248\n",
            "EmbeddingLSTM Epoch [28/100], Avg Loss: 0.4396\n",
            "EmbeddingLSTM Epoch [29/100], Avg Loss: 0.4594\n",
            "EmbeddingLSTM Epoch [30/100], Avg Loss: 0.6443\n",
            "EmbeddingLSTM Epoch [31/100], Avg Loss: 0.7864\n",
            "EmbeddingLSTM Epoch [32/100], Avg Loss: 0.4789\n",
            "EmbeddingLSTM Epoch [33/100], Avg Loss: 0.2464\n",
            "EmbeddingLSTM Epoch [34/100], Avg Loss: 0.1412\n",
            "EmbeddingLSTM Epoch [35/100], Avg Loss: 0.0877\n",
            "EmbeddingLSTM Epoch [36/100], Avg Loss: 0.0590\n",
            "EmbeddingLSTM Epoch [37/100], Avg Loss: 0.0435\n",
            "EmbeddingLSTM Epoch [38/100], Avg Loss: 0.0421\n",
            "EmbeddingLSTM Epoch [39/100], Avg Loss: 0.1213\n",
            "EmbeddingLSTM Epoch [40/100], Avg Loss: 0.1424\n",
            "EmbeddingLSTM Epoch [41/100], Avg Loss: 0.0717\n",
            "EmbeddingLSTM Epoch [42/100], Avg Loss: 0.0372\n",
            "EmbeddingLSTM Epoch [43/100], Avg Loss: 0.0208\n",
            "EmbeddingLSTM Epoch [44/100], Avg Loss: 0.0130\n",
            "EmbeddingLSTM Epoch [45/100], Avg Loss: 0.0091\n",
            "EmbeddingLSTM Epoch [46/100], Avg Loss: 0.0277\n",
            "EmbeddingLSTM Epoch [47/100], Avg Loss: 0.1807\n",
            "EmbeddingLSTM Epoch [48/100], Avg Loss: 0.0821\n",
            "EmbeddingLSTM Epoch [49/100], Avg Loss: 0.0292\n",
            "EmbeddingLSTM Epoch [50/100], Avg Loss: 0.0125\n",
            "EmbeddingLSTM Epoch [51/100], Avg Loss: 0.0073\n",
            "EmbeddingLSTM Epoch [52/100], Avg Loss: 0.0047\n",
            "EmbeddingLSTM Epoch [53/100], Avg Loss: 0.0034\n",
            "EmbeddingLSTM Epoch [54/100], Avg Loss: 0.0025\n",
            "EmbeddingLSTM Epoch [55/100], Avg Loss: 0.0018\n",
            "EmbeddingLSTM Epoch [56/100], Avg Loss: 0.0013\n",
            "EmbeddingLSTM Epoch [57/100], Avg Loss: 0.0010\n",
            "EmbeddingLSTM Epoch [58/100], Avg Loss: 0.0008\n",
            "EmbeddingLSTM Epoch [59/100], Avg Loss: 0.0006\n",
            "EmbeddingLSTM Epoch [60/100], Avg Loss: 0.2030\n",
            "EmbeddingLSTM Epoch [61/100], Avg Loss: 0.2664\n",
            "EmbeddingLSTM Epoch [62/100], Avg Loss: 0.0557\n",
            "EmbeddingLSTM Epoch [63/100], Avg Loss: 0.0162\n",
            "EmbeddingLSTM Epoch [64/100], Avg Loss: 0.0073\n",
            "EmbeddingLSTM Epoch [65/100], Avg Loss: 0.0045\n",
            "EmbeddingLSTM Epoch [66/100], Avg Loss: 0.0032\n",
            "EmbeddingLSTM Epoch [67/100], Avg Loss: 0.0025\n",
            "EmbeddingLSTM Epoch [68/100], Avg Loss: 0.0019\n",
            "EmbeddingLSTM Epoch [69/100], Avg Loss: 0.0014\n",
            "EmbeddingLSTM Epoch [70/100], Avg Loss: 0.0011\n",
            "EmbeddingLSTM Epoch [71/100], Avg Loss: 0.0008\n",
            "EmbeddingLSTM Epoch [72/100], Avg Loss: 0.0006\n",
            "EmbeddingLSTM Epoch [73/100], Avg Loss: 0.0005\n",
            "EmbeddingLSTM Epoch [74/100], Avg Loss: 0.2824\n",
            "EmbeddingLSTM Epoch [75/100], Avg Loss: 0.1761\n",
            "EmbeddingLSTM Epoch [76/100], Avg Loss: 0.0402\n",
            "EmbeddingLSTM Epoch [77/100], Avg Loss: 0.0119\n",
            "EmbeddingLSTM Epoch [78/100], Avg Loss: 0.0054\n",
            "EmbeddingLSTM Epoch [79/100], Avg Loss: 0.0035\n",
            "EmbeddingLSTM Epoch [80/100], Avg Loss: 0.0026\n",
            "EmbeddingLSTM Epoch [81/100], Avg Loss: 0.0020\n",
            "EmbeddingLSTM Epoch [82/100], Avg Loss: 0.0015\n",
            "EmbeddingLSTM Epoch [83/100], Avg Loss: 0.0012\n",
            "EmbeddingLSTM Epoch [84/100], Avg Loss: 0.0009\n",
            "EmbeddingLSTM Epoch [85/100], Avg Loss: 0.0007\n",
            "EmbeddingLSTM Epoch [86/100], Avg Loss: 0.0005\n",
            "EmbeddingLSTM Epoch [87/100], Avg Loss: 0.0004\n",
            "EmbeddingLSTM Epoch [88/100], Avg Loss: 0.0003\n",
            "EmbeddingLSTM Epoch [89/100], Avg Loss: 0.2674\n",
            "EmbeddingLSTM Epoch [90/100], Avg Loss: 0.1639\n",
            "EmbeddingLSTM Epoch [91/100], Avg Loss: 0.0347\n",
            "EmbeddingLSTM Epoch [92/100], Avg Loss: 0.0095\n",
            "EmbeddingLSTM Epoch [93/100], Avg Loss: 0.0044\n",
            "EmbeddingLSTM Epoch [94/100], Avg Loss: 0.0030\n",
            "EmbeddingLSTM Epoch [95/100], Avg Loss: 0.0022\n",
            "EmbeddingLSTM Epoch [96/100], Avg Loss: 0.0017\n",
            "EmbeddingLSTM Epoch [97/100], Avg Loss: 0.0013\n",
            "EmbeddingLSTM Epoch [98/100], Avg Loss: 0.0010\n",
            "EmbeddingLSTM Epoch [99/100], Avg Loss: 0.0008\n",
            "EmbeddingLSTM Epoch [100/100], Avg Loss: 0.0006\n",
            "EmbeddingLSTM Training Time: 2196.70s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Poems\n",
        "seed_text = \"I work hard for everything\"\n",
        "print(\"\\nGenerated Poem (OneHotRNN):\", generate_poem(onehot_model, seed_text, model_type = \"OneHotRNN\"))\n",
        "print(\"\\nGenerated Poem (EmbeddingLSTM):\", generate_poem(embedding_model, seed_text, model_type = \"EmbeddingLSTM\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGqmXakH2uMZ",
        "outputId": "faf8fb10-3607-476f-9f18-b4a00255ac55"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Poem (OneHotRNN): I work hard for everything The young mother you hear the whole, The cries, curses, roar, the plaudits for well-aim'd shots, The ambulanza slowly passing trailing its red drip, Workmen searching after damages, making indispensable repairs, The fall of grenades through the rent roof, the fan-shaped explosion, The whizz of limbs, heads, stone, wood, iron,\n",
            "\n",
            "Generated Poem (EmbeddingLSTM): I work hard for everything must not give out these are not from bed and meeting They listening ears and her and accept And as he as my thrush Thou madest O perfect day: Whereon shall no man work, A fail in the same of the day,) Far from the settlements studying the print of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HshlDVjPR_i6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}